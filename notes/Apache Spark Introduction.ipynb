{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Apache Spark\n",
    "Spark applications run as independent sets of processes on a cluster, coordinated by the **SparkContext object** in your main program (called the driver program).\n",
    "\n",
    "![Spark Architecture](http://spark.apache.org/docs/latest/img/cluster-overview.png)\n",
    "\n",
    "SparkContext allocate resources across applications. \n",
    "\n",
    "Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. \n",
    "\n",
    "Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors. \n",
    "\n",
    "Finally, SparkContext sends tasks to the executors to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x3bf27b8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive programming: is the procedure of writing parts of a program while it is **already active**. The Jupyter Notebook will be the frontend for our **active program**.\n",
    "\n",
    "For interactive programming we will have:\n",
    "* A Jupyter/IPython notebook: where we run Python code\n",
    "* PySparkShell application UI: to monitor Spark Cluster\n",
    "\n",
    "## Monitoring Spark Jobs\n",
    "\n",
    "Every SparkContext launches its own instance of Web UI which is available at http://[master]:4040 by default.\n",
    "\n",
    "Web UI comes with the following tabs:\n",
    "\n",
    "    * Jobs\n",
    "    * Stages\n",
    "    * Storage with RDD size and memory use\n",
    "    * Environment\n",
    "    * Executors\n",
    "    * SQL\n",
    "\n",
    "This information is available only until the application is running by default. \n",
    "\n",
    "### Jobs\n",
    "* Job id\n",
    "* Description\n",
    "* Submission dat\n",
    "* Job Duration\n",
    "* Stages\n",
    "* Tasks\n",
    "![Jobs Page](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/images/spark-webui-jobs.png)\n",
    "\n",
    "### Stages\n",
    "** What is a Stage? **: \n",
    "\n",
    "A stage is a physical unit of execution. It is a step in a physical execution plan.\n",
    "\n",
    "A stage is a set of parallel tasks, one per partition of an RDD, that compute partial results of a function executed as part of a Spark job.\n",
    "\n",
    "![Stages](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/diagrams/stage-tasks.png)\n",
    "\n",
    "\n",
    "\n",
    "In other words, a Spark job is a computation with that computation sliced into stages.\n",
    "\n",
    "A stage is uniquely identified by id. When a stage is created, DAGScheduler increments internal counter nextStageId to track the number of stage submissions.\n",
    "\n",
    "A stage can only work on the partitions of a single RDD (identified by rdd), but can be associated with many other dependent parent stages (via internal field parents), with the boundary of a stage marked by shuffle dependencies.\n",
    "\n",
    "![Stages](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/images/spark-webui-stages-completed.png)\n",
    "\n",
    "### Storage\n",
    "\n",
    "Storage page permit us to see how RDD are partitioned across the cluster.\n",
    "\n",
    "![Storage Page](https://github.com/f-guitart/data_mining/blob/master/notes/img/apache_spark_storage_page.png?raw=true)\n",
    "\n",
    "### Environment \n",
    "\n",
    "This tab shows configuration and variables used in Apache Spark execution.\n",
    "\n",
    "### Executors\n",
    "\n",
    "In this tab, we can see information about executors available in the cluster. \n",
    "\n",
    "We can have relevant information about CPU and Memory, as wel as RDD storage.\n",
    "\n",
    "We can also have information about executed tasks.\n",
    "\n",
    "![Executors Page](https://github.com/f-guitart/data_mining/blob/master/notes/img/apache_spark_executors_page.png?raw=true)\n",
    "\n",
    "### SQL\n",
    "\n",
    "By default, it displays all SQL query executions. However, after a query has been selected, the SQL tab displays the details of the SQL query execution.\n",
    "\n",
    "## Main Spark Concepts\n",
    "\n",
    "### Partitions\n",
    "Sparkâ€™s basic abstraction is the **Resilient Distributed Dataset, or RDD**.  \n",
    "\n",
    "That fragmentation is what enables Spark to execute in parallel, and the level of fragmentation is a function of the number of **partitions** of your RDD.  \n",
    "\n",
    "### Caching\n",
    "\n",
    "You will often hear: \"Apache handles all data in memory\". \n",
    "\n",
    "This is tricky and here's where the magic relies. Most of the time you will be working with metadata not with all the data, and computations are only left for the time that you need the results.\n",
    "\n",
    "Storing that results or leaving them to compute them again has a high impact in response times. When you store the results, it is said to be **catching the RDD**.\n",
    "\n",
    "### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[992023968016L, 984095744256L, 976215137296L, 968381956096L, 960596010000L]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
