{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Apache Spark?\n",
    "* distributed framework\n",
    "* in-memory data structures \n",
    "* data processing\n",
    "* it imptoves (most of the times) Hadoop workloads\n",
    "\n",
    "Spark enables data scientists to tackle problems with larger data sizes than they could before with tools like R or Pandas\n",
    "\n",
    "## First Steps with Apache Spark Interactive Programming\n",
    "\n",
    "First of all check that PySpark is running propertly. You can check if PySpark is correctly loaded:\n",
    "In case it is not, you can follow these posts:\n",
    "    * Windows (IPython): http://jmdvinodjmd.blogspot.com.es/2015/08/installing-ipython-notebook-with-apache.html \n",
    "    * Windows (Jupyter): http://www.ithinkcloud.com/tutorials/tutorial-on-how-to-install-apache-spark-on-windows/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x3b737b8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note is that with Spark all computation is parallelized by means of distributed data structures that are spreadd through the cluster. These collections are called Resilient Distributed Datasets (RDD). We will talk more about RDD, as they are the main piece in Spark.\n",
    "\n",
    "As we have successfully loaded the Spark Context, we are ready to do some interactive analysis. We can read a simple file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"../data/people.csv\")\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very simple first example, where we create an RDD (variable lines) and then we apply some operations (count and first) in a parallel manner. It has to be noted, that as we are running all our examples in a single computer the parallelization is not applied. \n",
    "\n",
    "In the next section we will cover the core Spark concepts that allow Spark users to do parallel computation.\n",
    "\n",
    "## Core Spark Concepts\n",
    "\n",
    "We will talk about **Spark applications** that are in charge of loading data and aplying some distributed computation over it. Every application has a **driver program** that launches parallel operations to the cluster. In the case of interactive programming, the driver program is the shell (or Notebook) itself.\n",
    "\n",
    "The \"access point\" to Spark from the driver program is the Spark Context object. As we have previously seen, using the referenced documentation, the sc object, is automatically loaded in the notebook.\n",
    "\n",
    "Once we have an Spark Context we can use it to build RDDs. In the previous examples we used sc.textFile() to represent the lines of the textFile. Then we run different operations over the RDD lines. \n",
    "\n",
    "To run these operations over RDDs, driver programs manage different nodes called executors. For example, for the count operation, it is possible to run count in different ranges of the file. \n",
    "\n",
    "Spark's API allows passing functions to its operators to run them on the cluster. For example, we could extend our example by filtering the lines in the file that contain a word, such as individuum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"../data/people.csv\")\n",
    "filtered_lines = lines.filter(lambda line: \"individuum\" in line)\n",
    "filtered_lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Basics\n",
    "\n",
    "An RDD can be defined as adistributed collection of elements. All work done with Spark can be summarized as **creating**, **trasnforming** and **applying** operations over RDDs to compute a result. Under the hood, Spark automatically **distributes the data contained in RDDs** across your cluster and **parallelizes the operations** you perform on them.\n",
    "\n",
    "RDD properties:\n",
    "* it is an **immutable distributed** collection of objects\n",
    "* it is split into multiple **partitions**\n",
    "* it is computed on different nodes of the cluster\n",
    "* it can contain any type of Python object (user defined ones included)\n",
    "\n",
    "An RDD can be created in **two ways**:\n",
    "1. loading an external dataset\n",
    "2. distributing a collection of objects in the driver program\n",
    "\n",
    "We have already seen the two ways of creating an RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loading an external dataset\n",
    "lines = sc.textFile(\"../data/people.csv\")\n",
    "print type(lines)\n",
    "# applying a transformation to an existing RDD\n",
    "filtered_lines = lines.filter(lambda line: \"individuum\" in line)\n",
    "print type(filtered_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that once we have an RDD, we can run **two kind of operations**:\n",
    "* **transformations**: construct a new RDD from a previous one. For example, by filtering lines RDD we create a new RDD that holds the lines that contain \"individuum\" string. Note that the returning result is an RDD.\n",
    "* **actions**: *compute* a result based on an RDD, and returns the result to the driver program or stores it to an external storage system (e.g. HDFS). Note that the returning result is not an RDD but another kind of variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "action_result = lines.first()\n",
    "print type(action_result)\n",
    "action_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations and actions are very different because of the way Spark computes RDDs. \n",
    "\n",
    "Transformations are defined in a **lazy** mannerm this is they are **only computed once they are used in an action**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filtered_lines is not computed until the next action is applied over it\n",
    "# it make sense when working with big data sets, as it is not necessary to \n",
    "# transform the whole RDD to get an action over a subset\n",
    "# Spark doesn't even reads the complete file!\n",
    "filtered_lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drawback is that Spark  **recomputes** again the RDD at **each action application**. \n",
    "\n",
    "This means that the computing effort over an already computed RDD may be lost. \n",
    "\n",
    "To mitigate this drwaback, the user can take de decision of **persisting** the RDD after computing it the first time, **Spark will store the RDD contents in memory**  (partitioned across the machines in your cluster), and reuse them in future actions. \n",
    "\n",
    "**Persisting RDDs on disk** instead of memory is also possible.\n",
    "\n",
    "Let's see an example on the impact of persisting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count 1: 17.3249998093\n",
      "Word count 2: 16.6799998283\n",
      "Word count persisted 1: 30.1779999733\n",
      "Word count persisted 2: 14.0679998398\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "lines = sc.textFile(\"../data/REFERENCE/*\")\n",
    "lines_nonempty = lines.filter( lambda x: len(x) > 0 )\n",
    "words = lines_nonempty.flatMap(lambda x: x.split())\n",
    "words_persisted = lines_nonempty.flatMap(lambda x: x.split())\n",
    "\n",
    "t1 = time.time()\n",
    "words.count()\n",
    "print \"Word count 1:\",time.time() - t1\n",
    "\n",
    "t1 = time.time()\n",
    "words.count()\n",
    "print \"Word count 2:\",time.time() - t1\n",
    "\n",
    "t1 = time.time()\n",
    "words_persisted.persist()\n",
    "words_persisted.count()\n",
    "print \"Word count persisted 1:\",time.time() - t1\n",
    "\n",
    "t1 = time.time()\n",
    "words_persisted.count()\n",
    "print \"Word count persisted 2:\", time.time() - t1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Operations\n",
    "\n",
    "We have already seen that RDDs have two basic operations: **transformations** and **actions**.\n",
    "\n",
    "**Transformations** are operations that return a new RDD. *Examples:* filter, map.\n",
    "\n",
    "Remember that , transformed RDDs are **computed lazily**, only when you use them in an action.\n",
    "\n",
    "Lazy evaluation means that when we call a transformation on an RDD (for instance, calling map()), the operation is **not immediately performed**. \n",
    "\n",
    "Instead, Spark internally records **metadata** to indicate that this operation has been requested. \n",
    "\n",
    "**Loading data** into an RDD is lazily evaluated in the same way trans formations are. So, when we call sc.textFile(), the data is **not loaded** until it is necessary. \n",
    "\n",
    "As with transformations, the operation (in this case, reading the data) can occur multiple times. Take in mind that transformations **DO HAVE** impact over computation time.\n",
    "\n",
    "Many transformations are **element-wise**; that is, they work on one element at a time; but this is not true for all transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"../data/REFERENCE/*\")\n",
    "lines_nonempty = lines.filter( lambda x: len(x) > 0 )\n",
    "words = lines_nonempty.flatMap(lambda x: x.split())\n",
    "words_persisted = lines_nonempty.flatMap(lambda x: x.split())\n",
    "words.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* filter applies the lambda function to each line in lines RDD, only lines that accmolish the condition that the length is creater than zero are in lines_nonempty variable (**this RDD is not computed yet!**)\n",
    "* flatMap applies the lambda function to each element of the RDD and then the result is flattened (i.e. a list of lists would be converted to a simple list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actions** are operations that return an object to the driver program or wirte to external storage, they kick a computation. *Examples:* first, count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "words.count()\n",
    "print \"Word count 1:\",time.time() - t1\n",
    "\n",
    "t1 = time.time()\n",
    "words.count()\n",
    "print \"Word count 2:\",time.time() - t1\n",
    "\n",
    "t1 = time.time()\n",
    "words_persisted.persist()\n",
    "words_persisted.count()\n",
    "print \"Word count persisted 1:\",time.time() - t1\n",
    "\n",
    "t1 = time.time()\n",
    "words_persisted.count()\n",
    "print \"Word count persisted 2:\", time.time() - t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions are the operations that return a **final value** to the driver program or write data to an external storage system. \n",
    "\n",
    "Actions **force the evaluation** of the transformations required for the **RDD** they were called on, since they need to actually produce output.\n",
    "\n",
    "Returning to the previous example, until we call count over words and words persisted, the RDD are not computed. See that we persisted words_persisted, and util its second computation we cannot see the impact of persisting that RDD in memory.\n",
    "\n",
    "If we want to see a part of the RDD, we can use take, and to have the full RDD we can use collect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"../data/people.csv\")\n",
    "print \"Three elements\", lines.take(3)\n",
    "print \"The whole RDD\", lines.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing functions to Spark\n",
    "\n",
    "Most of Spark’s transformations, and some of its actions, depend on **passing in functions** that are used by Spark to **compute** data.\n",
    "\n",
    "In Python, we have three options for passing functions into Spark. \n",
    " * For shorter functions, we can pass in lambda expressions\n",
    " * We can pass in top-level functions, or \n",
    " * Locally defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"../data/people.csv\")\n",
    "\n",
    "first_cells = lines.map(lambda x: x.split(\",\")[0])\n",
    "print first_cells.collect()\n",
    "\n",
    "# how to pass estra arguments\n",
    "def get_cell(x):\n",
    "    return x.split(\",\")[0]\n",
    "first_cells = lines.map(get_cell)\n",
    "print first_cells.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with common Spark transformations\n",
    "\n",
    "The two most common transformations you will likely be using are map() and filter(). \n",
    "\n",
    "The **map()** transformation takes in a function and applies it to each element in the RDD with the result of the function being the new value of each element in the resulting RDD. \n",
    "\n",
    "The **filter()** transformation takes in a function and returns an RDD that only has elements that pass the filter() function.\n",
    "\n",
    "Sometimes ** map() ** returns nested lists, to flattern these nested lists we can use ** flatMap() **. So, ** flatMap() ** is called individually for each element in our input RDD. Instead of returning a single element, we return an iterator with our return values. Rather than producing an RDD of iterators, we get back an RDD that consists of the elements from all of the iterators.\n",
    "\n",
    "### Set operations\n",
    "\n",
    "* **distinct()** transformation to produce a new RDD with only distinct items. Note that distinct() is expensive, however, as it requires shuffling all the data over the network to ensure that we receive only one copy of each element. \n",
    "\n",
    "* **RDD.union(other)** back an RDD consisting of the data from both sources. Unlike the mathematical union(), if there are duplicates in the input RDDs, the result of Spark’s union() will contain duplicates (which we can fix if desired with distinct()).\n",
    "\n",
    "* **RDD.intersection(other)**  returns only elements in both RDDs. intersection() also removes all duplicates (including duplicates from a single RDD) while running. While intersection() and union() are two similar concepts, the performance of intersection() is much worse since it requires a shuffle over the network to identify common elements.\n",
    "\n",
    "* ** RDD.subtract(other)** function takes in another RDD and returns an RDD that has only values present in the first RDD and not the second RDD. Like intersection(), it performs a shuffle.\n",
    "\n",
    "* ** RDD.cartesian(other) ** transformation returns all possible pairs of (a,b) where a is in the source RDD and b is in the other RDD. The Cartesian product can be useful when we wish to consider the similarity between all possible pairs, such as computing every user’s expected interest in each offer. We can also take the Cartesian product of an RDD with itself, which can be useful for tasks like user similarity. Be warned, however, that the Cartesian product is very expensive for large RDDs.\n",
    "\n",
    "### Actions\n",
    "\n",
    "* **reduce():** which takes a function that operates on two elements of the type in your RDD and returns a new element of the same type. \n",
    "\n",
    "* **aggreggate():** takes an initial zero value of the type we want to return. We then supply a function to combine the elements from our RDD with the accumulator. Finally, we need to supply a second function to merge two accumulators, given that each node accumulates its own results locally. To know more:\n",
    "    * http://stackoverflow.com/questions/28240706/explain-the-aggregate-functionality-in-spark\n",
    "    * http://atlantageek.com/2015/05/30/python-aggregate-rdd/\n",
    "    \n",
    "* **collect():** returns the entire RDD’s contents. collect() is commonly used in unit tests where the entire contents of the RDD are expected to fit in memory, as that makes it easy to compare the value of our RDD with our expected result.\n",
    "\n",
    "* **take(n):** returns n elements from the RDD and attempts to minimize the number of partitions it accesses, so it may represent a biased collection\n",
    "\n",
    "* **top():** will use the default ordering on the data, but we can supply our own comparison function to extract the top elements. \n",
    "\n",
    "\n",
    "### Exercices\n",
    "\n",
    "** Exercice 1: ** Download all books, from books.csv using the map function.\n",
    "\n",
    "** Exercice 2: ** Identify transformations and actions. When the returned data is calculated?\n",
    "\n",
    "** Exercice 3: ** Imagine that you only want to download Dickens books, how would you do that? Which is the impact of not persisting dickens_books_content?\n",
    "\n",
    "** Exercice 4: ** Use flatMap() in the resulting RDD of the previous exercice, how the result is different?\n",
    "\n",
    "** Exercice 5: ** You want to know the different books authors there are.\n",
    "\n",
    "** Exercice 6: ** Return Poe's and Dickens' books URLs (use union function).\n",
    "\n",
    "** Exercice 7: ** Return the list of books without Dickens' and Poe's books.\n",
    "\n",
    "** Exercice 8: ** Count the number of books using reduce function.\n",
    "\n",
    "** Exercice 9: ** Compute the mean price of estates from csv containing Sacramento's estate price using aggregate function.\n",
    "\n",
    "** Exercice 10: ** Get top 5 highest and lowest prices in Sacramento estate's transactions\n",
    "\n",
    "** Answer 1: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'http://www.textfiles.com/etext/REFERENCE/15-songs.txt', u'15-songs.txt', u'17619', u'A Civil War Songbook (January 1990)'], [u'http://www.textfiles.com/etext/REFERENCE/1776-va.rts', u'1776-va.rts', u'5907', u'The Virginia Declaration of Rights'], [u'http://www.textfiles.com/etext/REFERENCE/1mlkd11.txt', u'1mlkd11.txt', u'817486', u'\"Project Gutenberg: Martin Luther King\\'s \"\"I have a Dream\"\" Speech\"'], [u'http://www.textfiles.com/etext/REFERENCE/1st_than.txt', u'1st_than.txt', u'2979', u'\"The First Thanksgiving Proclomation', u' June 20', u' 1676\"'], [u'http://www.textfiles.com/etext/REFERENCE/2sqrt10a.txt', u'2sqrt10a.txt', u'5262079', u'\"Project Gutenberg: The Square Root of Two', u' to 5 Million digits\"'], [u'http://www.textfiles.com/etext/REFERENCE/32pri10.txt', u'32pri10.txt', u'247391', u'Project Gutenberg: The 32nd Mersenne prime'], [u'http://www.textfiles.com/etext/REFERENCE/all11.txt', u'all11.txt', u'85580', u'Project Gutenberg: The Declaration of Independence of The United States of America'], [u'http://www.textfiles.com/etext/REFERENCE/berne10.txt', u'berne10.txt', u'63397', u'Project Gutenberg: The Berne Copyright Convention'], [u'http://www.textfiles.com/etext/REFERENCE/bill11.txt', u'bill11.txt', u'2974', u'The United States Bill of Rights'], [u'http://www.textfiles.com/etext/REFERENCE/charlrsl.txt', u'charlrsl.txt', u'8446', u'\"The Charlotte Town Resolves: Resolves Adopted in Charlotte Town', u' North Carolina', u' May 31', u' 1775\"']]\n",
      "Newsgroups: freenet.shrine.songs\n",
      "From: aa300 (Jerry Murphy)\n",
      "Subject: Songs from the Civil War\n",
      "Dat\n"
     ]
    }
   ],
   "source": [
    "import urllib3\n",
    "\n",
    "def download_file(csv_line):\n",
    "    link = csv_line[0]\n",
    "    http = urllib3.PoolManager()\n",
    "    r = http.request('GET', link, preload_content=False)\n",
    "    response = r.read()\n",
    "    return response\n",
    "    \n",
    "books_info = sc.textFile(\"../data/books.csv\").map(lambda x: x.split(\",\"))\n",
    "print books_info.take(10)\n",
    "\n",
    "books_content = books_info.map(download_file)\n",
    "print books_content.take(1)[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer 2: **\n",
    "If we consider the text readong as a transformation...\n",
    "Transformations:\n",
    "* books_info = sc.textFile(\"../data/books.csv\").map(lambda x: x.split(\",\"))\n",
    "* books_content = books_info.map(lambda x: download_file(x[0]))\n",
    "\n",
    "Actions:\n",
    "* print books_info.take(10)\n",
    "* print books_content.take(1)[0][:100]\n",
    "\n",
    "Computation is carried out in acions. In this case we take advantage of it, as for downloading data we only apply the function to one element of the books_content RDD\n",
    "\n",
    "** Answer 3: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_dickens(csv_line):\n",
    "    link = csv_line[0]\n",
    "    t = re.match(\"http://www.textfiles.com/etext/AUTHORS/DICKENS/\",link)\n",
    "    return t != None\n",
    "\n",
    "dickens_books_info = books_info.filter(is_dickens)\n",
    "print dickens_books_info.take(4)\n",
    "\n",
    "dickens_books_content = dickens_books_info.map(download_file)\n",
    "\n",
    "# take into considertaion that each time an action is performed over dickens_book_content, the file is downloaded\n",
    "# this has a big impact into calculations\n",
    "print dickens_books_content.take(2)[1][:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer 4: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flat_content = dickens_books_info.flatMap(lambda x: x)\n",
    "print flat_content.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer 5: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_author(csv_line):\n",
    "    link = csv_line[0]\n",
    "    t = re.match(\"http://www.textfiles.com/etext/AUTHORS/(\\w+)/\",link)\n",
    "    if t:\n",
    "        return t.group(1)\n",
    "    return u'UNKNOWN'\n",
    "\n",
    "authors = books_info.map(get_author)\n",
    "authors.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Answer 6 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'POE', u'http://www.textfiles.com/etext/AUTHORS/POE/poe-al-425.txt'),\n",
       " (u'POE', u'http://www.textfiles.com/etext/AUTHORS/POE/poe-conqueror-676.txt'),\n",
       " (u'POE', u'http://www.textfiles.com/etext/AUTHORS/POE/poe-eldorado-436.txt'),\n",
       " (u'POE',\n",
       "  u'http://www.textfiles.com/etext/AUTHORS/POE/poe-metzengerstein-557.txt'),\n",
       " (u'POE', u'http://www.textfiles.com/etext/AUTHORS/POE/poe-never-562.txt'),\n",
       " (u'POE', u'http://www.textfiles.com/etext/AUTHORS/POE/poe-premature-700.txt'),\n",
       " (u'POE', u'http://www.textfiles.com/etext/AUTHORS/POE/poe-x-726.txt')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_author_and_link(csv_line):\n",
    "    link = csv_line[0]\n",
    "    t = re.match(\"http://www.textfiles.com/etext/AUTHORS/(\\w+)/\",link)\n",
    "    if t:\n",
    "        return (t.group(1), link)\n",
    "    return (u'UNKNOWN',link)\n",
    "\n",
    "authors_links = books_info.map(get_author_and_link)\n",
    "\n",
    "# not very efficient\n",
    "dickens_books = authors_links.filter(lambda x: x[0]==\"DICKENS\")\n",
    "poes_books = authors_links.filter(lambda x: x[0]==\"POE\")\n",
    "\n",
    "poes_dickens_books = poes_books.union(dickens_books)\n",
    "poes_dickens_books.sample(True,0.05).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer 7 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "authors_links.subtract(poes_dickens_books).map(lambda x: x[0]).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer 8 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "authors_links.map(lambda x: 1).reduce(lambda x,y: x+y) == authors_links.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sacramento_estate_csv = sc.textFile(\"../data/Sacramentorealestatetransactions.csv\")\n",
    "header = sacramento_estate_csv.first()\n",
    "\n",
    "sacramento_estate = sacarmento_estate_csv.filter(lambda x: x != header)\\\n",
    "        .map(lambda x: x.split(\",\"))\\\n",
    "        .map(lambda x: int(x[9]))\n",
    "\n",
    "seqOp = (lambda x,y: (x[0] + y, x[1] + 1))\n",
    "combOp = (lambda x,y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "total_sum, number = sacramento_estate.aggregate((0,0),seqOp,combOp)\n",
    "mean = float(total_sum)/number\n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sacramento_estate.top(5)\n",
    "print sacramento_estate.top(5, key=lambda x: -x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
