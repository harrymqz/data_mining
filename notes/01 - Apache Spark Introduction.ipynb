{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Apache Spark\n",
    "Spark applications run as independent sets of processes on a cluster, coordinated by the **SparkContext object** in your main program (called the driver program).\n",
    "\n",
    "![Spark Architecture](http://spark.apache.org/docs/latest/img/cluster-overview.png)\n",
    "\n",
    "SparkContext allocate resources across applications. \n",
    "\n",
    "Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. \n",
    "\n",
    "Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors. \n",
    "\n",
    "Finally, SparkContext sends tasks to the executors to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x3bf27b8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive programming: is the procedure of writing parts of a program while it is **already active**. The Jupyter Notebook will be the frontend for our **active program**.\n",
    "\n",
    "For interactive programming we will have:\n",
    "* A Jupyter/IPython notebook: where we run Python code\n",
    "* PySparkShell application UI: to monitor Spark Cluster\n",
    "\n",
    "## Monitoring Spark Jobs\n",
    "\n",
    "Every SparkContext launches its own instance of Web UI which is available at http://[master]:4040 by default.\n",
    "\n",
    "Web UI comes with the following tabs:\n",
    "\n",
    "    * Jobs\n",
    "    * Stages\n",
    "    * Storage with RDD size and memory use\n",
    "    * Environment\n",
    "    * Executors\n",
    "    * SQL\n",
    "\n",
    "This information is available only until the application is running by default. \n",
    "\n",
    "### Jobs\n",
    "* Job id\n",
    "* Description\n",
    "* Submission dat\n",
    "* Job Duration\n",
    "* Stages\n",
    "* Tasks\n",
    "![Jobs Page](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/images/spark-webui-jobs.png)\n",
    "\n",
    "### Stages\n",
    "** What is a Stage? **: \n",
    "\n",
    "A stage is a physical unit of execution. It is a step in a physical execution plan.\n",
    "\n",
    "A stage is a set of parallel tasks, one per partition of an RDD, that compute partial results of a function executed as part of a Spark job.\n",
    "\n",
    "![Stages](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/diagrams/stage-tasks.png)\n",
    "\n",
    "\n",
    "\n",
    "In other words, a Spark job is a computation with that computation sliced into stages.\n",
    "\n",
    "A stage is uniquely identified by id. When a stage is created, DAGScheduler increments internal counter nextStageId to track the number of stage submissions.\n",
    "\n",
    "A stage can only work on the partitions of a single RDD (identified by rdd), but can be associated with many other dependent parent stages (via internal field parents), with the boundary of a stage marked by shuffle dependencies.\n",
    "\n",
    "![Stages](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/images/spark-webui-stages-completed.png)\n",
    "\n",
    "### Storage\n",
    "\n",
    "Storage page permit us to see how RDD are partitioned across the cluster.\n",
    "\n",
    "![Storage Page](https://github.com/f-guitart/data_mining/blob/master/notes/img/apache_spark_storage_page.png?raw=true)\n",
    "\n",
    "### Environment \n",
    "\n",
    "This tab shows configuration and variables used in Apache Spark execution.\n",
    "\n",
    "### Executors\n",
    "\n",
    "In this tab, we can see information about executors available in the cluster. \n",
    "\n",
    "We can have relevant information about CPU and Memory, as wel as RDD storage.\n",
    "\n",
    "We can also have information about executed tasks.\n",
    "\n",
    "![Executors Page](https://github.com/f-guitart/data_mining/blob/master/notes/img/apache_spark_executors_page.png?raw=true)\n",
    "\n",
    "### SQL\n",
    "\n",
    "By default, it displays all SQL query executions. However, after a query has been selected, the SQL tab displays the details of the SQL query execution.\n",
    "\n",
    "## Main Spark Concepts\n",
    "\n",
    "### Partitions\n",
    "Spark’s basic abstraction is the **Resilient Distributed Dataset, or RDD**.  \n",
    "\n",
    "That fragmentation is what enables Spark to execute in parallel, and the level of fragmentation is a function of the number of **partitions** of your RDD.  \n",
    "\n",
    "### Caching\n",
    "\n",
    "You will often hear: \"Apache handles all data in memory\". \n",
    "\n",
    "This is tricky and here's where the magic relies. Most of the time you will be working with metadata not with all the data, and computations are only left for the time that you need the results.\n",
    "\n",
    "Storing that results or leaving them to compute them again has a high impact in response times. When you store the results, it is said to be **catching the RDD**.\n",
    "\n",
    "### Shuffling\n",
    "\n",
    "(*from: https://0x0fff.com/spark-architecture-shuffle/*)\n",
    "\n",
    "(*more about shuffling: https://spark.apache.org/docs/1.3.1/programming-guide.html#performance-impact*)\n",
    "\n",
    "(*best practices: https://robertovitillo.com/2015/06/30/spark-best-practices/*)\n",
    "\n",
    "There are many different tasks that require shuffling of the data across the cluster, for instance table join – to join two tables on the field “id”, you must be sure that all the data for the same values of “id” for both of the tables are stored in the same chunks. \n",
    "\n",
    "Imagine the tables with integer keys ranging from 1 to 1’000’000. By storing the data in same chunks I mean that for instance for both tables values of the key 1-100 are stored in a single partition/chunk, this way instead of going through the whole second table for each partition of the first one, we can join partition with partition directly, because we know that the key values 1-100 are stored only in these two partitions. To achieve this both tables should have the same number of partitions, this way their join would require much less computations. So now you can understand how important shuffling is.\n",
    "\n",
    "## Exercices\n",
    "\n",
    "(from: *http://blog.insightdatalabs.com/jupyter-on-apache-spark-step-by-step/*)\n",
    "\n",
    "** Exercice 1:** Check that SparkContext and Spark SQL Context are loaded in your current environment.\n",
    "\n",
    "** Exercice 2:** Create your first RDD with 20 partitions and check WebUI that the RDD has created a job, an stage and 20 partitions. The RDD must contain a list of 1000 integers starting from 0. Get the number of partitions using getNumPartitions().\n",
    "\n",
    "(Hint 1: you can use sc.parallelize)\n",
    "\n",
    "(Hint 2: check Spark API docs: http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.parallelize)\n",
    "\n",
    "** Exercice 3:** Get 5 elements of the RDD.\n",
    "\n",
    "** Exercice 4: ** Name the RDD as \"my_rdd\" and persist it into memory and disk serialized.\n",
    "\n",
    "** Exercice 5: ** Perform a  transformation to group the numbers into the lowest 100s and count the total frequency for each bin.\n",
    "\n",
    "** Exercice 6: ** Browse the WebUI. And:\n",
    "    * identify the RDD generated in Exercice X and its job\n",
    "    * identify the job in Exercice X \n",
    "    * check that the RDD has been cached\n",
    "    * identify the job in Exercice X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer 1: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is SpartContext loaded? True\n",
      "is SpartSQLContext loaded? True\n"
     ]
    }
   ],
   "source": [
    "## just check that sc variables is not \n",
    "print \"is SpartContext loaded?\", sc != ''\n",
    "print \"is SpartSQLContext loaded?\", sqlCtx != ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer 2: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([x for x in range(1000)],20)\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 4:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my_rdd ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:475"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.setName(\"my_rdd\").persist(StorageLevel.MEMORY_AND_DISK_SER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer 5: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 100),\n",
       " (800.0, 100),\n",
       " (100.0, 100),\n",
       " (200.0, 100),\n",
       " (300.0, 100),\n",
       " (400.0, 100),\n",
       " (500.0, 100),\n",
       " (600.0, 100),\n",
       " (900.0, 100),\n",
       " (700.0, 100)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda r: (round(r/100)*100, 1))\\\n",
    "   .reduceByKey(lambda x,y: x+y)\\\n",
    "   .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
